{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ROOAA Notebook_F.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "eyOQJmVUPMHJ",
        "fRIqLJzrPTUc",
        "gfL4FsTuPgR_",
        "qTYuUYJRPtlM"
      ],
      "authorship_tag": "ABX9TyNjxldsIC0JJudtvLvWqfXO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khaledAIVR/Rooaa/blob/master/ROOAA_Notebook_F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyOQJmVUPMHJ"
      },
      "source": [
        "# *YOLO object detection*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vG6HaVxmPAYK"
      },
      "source": [
        "cd /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vwygk5h_PBng"
      },
      "source": [
        "!git clone https://github.com/pjreddie/darknet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6oBMBq6PHGD"
      },
      "source": [
        "cd darknet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmC5vd6uPHe0"
      },
      "source": [
        "!sed -i 's/GPU=0/GPU=1/g' Makefile\r\n",
        "!make\r\n",
        "!wget https://pjreddie.com/media/files/yolov3.weights\r\n",
        "!pip install pydub\r\n",
        "!pip install opencv-python\r\n",
        "!pip install gtts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6NzbEyFPI8I"
      },
      "source": [
        "#yolo.py\r\n",
        "import os\r\n",
        "import glob\r\n",
        "import time\r\n",
        "import cv2 \r\n",
        "import numpy as np\r\n",
        "import imutils\r\n",
        "import subprocess\r\n",
        "\r\n",
        "def detect_and_locate_obj(path, LABELS, net, ln):\r\n",
        "  \r\n",
        "  # initialize\r\n",
        "  # load the image with imread()\r\n",
        "  imageSource = path\r\n",
        "  img = cv2.imread(imageSource)\r\n",
        "\r\n",
        "\r\n",
        "  COLORS = np.random.randint(0, 255, size=(len(LABELS), 3),\r\n",
        "    dtype=\"uint8\")\r\n",
        "\r\n",
        "\r\n",
        "  # grab the image dimensions and convert it to a blob\r\n",
        "  (H, W) = img.shape[:2]\r\n",
        "  # construct a blob from the input image and then perform a forward\r\n",
        "  # pass of the YOLO object detector, giving us our bounding boxes and\r\n",
        "  # associated probabilities\r\n",
        "  blob = cv2.dnn.blobFromImage(img, 1 / 255.0, (416, 416),\r\n",
        "  swapRB=True, crop=False)\r\n",
        "  net.setInput(blob)\r\n",
        "  layerOutputs = net.forward(ln)\r\n",
        "\r\n",
        "\r\n",
        "  # initialize our lists of detected bounding boxes, confidences, and\r\n",
        "  # class IDs, respectively\r\n",
        "  boxes = []\r\n",
        "  confidences = []\r\n",
        "  classIDs = []\r\n",
        "  detected = []\r\n",
        "  centers = []\r\n",
        "\r\n",
        "  # loop over each of the layer outputs\r\n",
        "  for output in layerOutputs:\r\n",
        "    # loop over each of the detections\r\n",
        "    for detection in output:\r\n",
        "        # extract the class ID and confidence (i.e., probability) of\r\n",
        "        # the current object detection\r\n",
        "        scores = detection[5:]\r\n",
        "        classID = np.argmax(scores)\r\n",
        "        confidence = scores[classID]\r\n",
        "\r\n",
        "\r\n",
        "        # filter out weak predictions by ensuring the detected\r\n",
        "        # probability is greater than the minimum probability\r\n",
        "        if confidence > 0.5:\r\n",
        "            # scale the bounding box coordinates back relative to the\r\n",
        "            # size of the image, keeping in mind that YOLO actually\r\n",
        "            # returns the center (x, y)-coordinates of the bounding\r\n",
        "            # box followed by the boxes' width and height\r\n",
        "            box = detection[0:4] * np.array([W, H, W, H])\r\n",
        "            (centerX, centerY, width, height) = box.astype(\"int\")\r\n",
        "\r\n",
        "\r\n",
        "            # use the center (x, y)-coordinates to derive the top and\r\n",
        "            # and left corner of the bounding box\r\n",
        "            x = int(centerX - (width / 2))\r\n",
        "            y = int(centerY - (height / 2))\r\n",
        "\r\n",
        "\r\n",
        "            # update our list of bounding box coordinates, confidences,\r\n",
        "            # and class IDs\r\n",
        "            boxes.append([x, y, int(width), int(height)])\r\n",
        "            confidences.append(float(confidence))\r\n",
        "            classIDs.append(classID)\r\n",
        "            centers.append((centerX, centerY))\r\n",
        "\r\n",
        "              \r\n",
        "  # apply non-maxima suppression to suppress weak, overlapping bounding\r\n",
        "  # boxes\r\n",
        "  idxs = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.3)\r\n",
        "\r\n",
        "  classes = []\r\n",
        "  locations = []\r\n",
        "  center_axis = []\r\n",
        "  # ensure at least one detection exists\r\n",
        "  if len(idxs) > 0: \r\n",
        "    # loop over the indexes we are keeping\r\n",
        "    for i in idxs.flatten():\r\n",
        "      # find  \r\n",
        "      centerX, centerY = centers[i][0], centers[i][1]\r\n",
        "\r\n",
        "\r\n",
        "      #-------------------------Locate objects-------------------------\"\"\"\r\n",
        "      if centerX <= W/3:\r\n",
        "        location = \"left \"\r\n",
        "      elif centerX <= (W/3 * 2):\r\n",
        "        location = \"center \"\r\n",
        "      else:\r\n",
        "        location = \"right \"\r\n",
        "\r\n",
        "      locations.append(location)\r\n",
        "      classes.append(LABELS[classIDs[i]])\r\n",
        "      center_axis.append([centerX, centerY])\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  return classes, locations, center_axis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRIqLJzrPTUc"
      },
      "source": [
        "# The Monocular Depth Estimation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htbY3sizPXoE"
      },
      "source": [
        "cd /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxEuzby0PZes"
      },
      "source": [
        "!git clone https://github.com/ialhashim/DenseDepth.git\r\n",
        "!wget https://s3-eu-west-1.amazonaws.com/densedepth/nyu.h5 -O ./DenseDepth/nyu.h5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPVctYXiPbqM"
      },
      "source": [
        "#layers.py\r\n",
        "from keras.engine.topology import Layer, InputSpec\r\n",
        "import keras.utils.conv_utils as conv_utils\r\n",
        "import tensorflow as tf\r\n",
        "import keras.backend as K\r\n",
        "\r\n",
        "class BilinearUpSampling2D(Layer):\r\n",
        "    def __init__(self, size=(2, 2), data_format=None, **kwargs):\r\n",
        "        super(BilinearUpSampling2D, self).__init__(**kwargs)\r\n",
        "        self.data_format = K.normalize_data_format(data_format)\r\n",
        "        self.size = conv_utils.normalize_tuple(size, 2, 'size')\r\n",
        "        self.input_spec = InputSpec(ndim=4)\r\n",
        "\r\n",
        "    def compute_output_shape(self, input_shape):\r\n",
        "        if self.data_format == 'channels_first':\r\n",
        "            height = self.size[0] * input_shape[2] if input_shape[2] is not None else None\r\n",
        "            width = self.size[1] * input_shape[3] if input_shape[3] is not None else None\r\n",
        "            return (input_shape[0],\r\n",
        "                    input_shape[1],\r\n",
        "                    height,\r\n",
        "                    width)\r\n",
        "        elif self.data_format == 'channels_last':\r\n",
        "            height = self.size[0] * input_shape[1] if input_shape[1] is not None else None\r\n",
        "            width = self.size[1] * input_shape[2] if input_shape[2] is not None else None\r\n",
        "            return (input_shape[0],\r\n",
        "                    height,\r\n",
        "                    width,\r\n",
        "                    input_shape[3])\r\n",
        "\r\n",
        "    def call(self, inputs):\r\n",
        "        input_shape = K.shape(inputs)\r\n",
        "        if self.data_format == 'channels_first':\r\n",
        "            height = self.size[0] * input_shape[2] if input_shape[2] is not None else None\r\n",
        "            width = self.size[1] * input_shape[3] if input_shape[3] is not None else None\r\n",
        "        elif self.data_format == 'channels_last':\r\n",
        "            height = self.size[0] * input_shape[1] if input_shape[1] is not None else None\r\n",
        "            width = self.size[1] * input_shape[2] if input_shape[2] is not None else None\r\n",
        "        \r\n",
        "        return tf.image.resize_images(inputs, [height, width], method=tf.image.ResizeMethod.BILINEAR, align_corners=True)\r\n",
        "\r\n",
        "    def get_config(self):\r\n",
        "        config = {'size': self.size, 'data_format': self.data_format}\r\n",
        "        base_config = super(BilinearUpSampling2D, self).get_config()\r\n",
        "        return dict(list(base_config.items()) + list(config.items()))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzLDFwL_Pfsg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfL4FsTuPgR_"
      },
      "source": [
        "# Import libs, define functions and load the two models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xqg_d16EPi9S"
      },
      "source": [
        "import os\r\n",
        "import glob\r\n",
        "import time\r\n",
        "import cv2 \r\n",
        "import numpy as np\r\n",
        "import imutils\r\n",
        "import subprocess\r\n",
        "import random    \r\n",
        "import socket\r\n",
        "import threading\r\n",
        "import IPython\r\n",
        "import portpicker\r\n",
        "\r\n",
        "from keras.models import load_model\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "from six.moves import SimpleHTTPServer\r\n",
        "from six.moves import socketserver\r\n",
        "from google.colab import output\r\n",
        "from google.colab import files\r\n",
        "from IPython.display import Audio\r\n",
        "from IPython.display import Image\r\n",
        "from IPython.display import display, Javascript\r\n",
        "from google.colab.output import eval_js\r\n",
        "from base64 import b64decode\r\n",
        "from gtts import gTTS\r\n",
        "#------------------------------------------------------\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\"\"\"/////////////////////////////////////////////////\"\"\"\r\n",
        "#danger_dictionary takes the label index as a key and the value is the estimated danger value\r\n",
        "danger_dict = {0: 9, 1: 8, 2: 10, 3: 9, 5: 10, 7: 10, 9: 10, 11: 10, 12: 8, 13: 6, 14: 4, 15: 4, 16: 5, 31: 5, 32: 7, 34: 7, 36: 7, 56: 8, 57: 8, 58: 8, 59: 8, 60: 8, 61: 8, 62: 5, 67: 5, 68: 7, 69: 7, 71: 7, 72: 6, 75: 5, 4: 0, 6: 0, 8: 0, 10: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 33: 0, 35: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 63: 0, 64: 0, 65: 0, 66: 0, 70: 0, 73: 0, 74: 0, 76: 0, 77: 0, 78: 0, 79: 0} \r\n",
        "labels_of_coco = [\"person\",\"bicycle\",\"car\",\"motorbike\",'aeroplane',\"bus\",\"train\",\"truck\",\"boat\",\"traffic light\",\"fire hydrant\",\"stop sign\",\"parking meter\",\"bench\",\"bird\",\"cat\",\"dog\",\"horse\",\"sheep\",\"cow\",\"elephant\",\"bear\",\"zebra\",\"giraffe\",\"backpack\",\"umbrella\",\"handbag\",\"tie\",\"suitcase\",\"frisbee\",\"skis\",\"snowboard\",\"sports ball\",\"kite\",\"baseball bat\",\"baseball glove\",\"skateboard\",\"surfboard\",\"tennis racket\",\"bottle\",\"wine glass\",\"cup\",\"fork\",\"knife\",\"spoon\",\"bowl\",\"banana\",\"apple\",\"sandwich\",\"orange\",\"broccoli\",\"carrot\",\"hot dog\",\"pizza\",\"donut\",\"cake\",\"chair\",\"sofa\",\"pottedplant\",\"bed\",\"diningtable\",\"toilet\",\"tvmonitor\",\"laptop\",\"mouse\",\"remote\",\"keyboard\",\"cell phone\",\"microwave\",\"oven\",\"toaster\",\"sink\",\"refrigerator\",\"book\",\"clock\",\"vase\",\"scissors\",\"teddy bear\",\"hair drier\",\"toothbrush\"]\r\n",
        "\r\n",
        "class filtration: \r\n",
        "    label = -1 \r\n",
        "    location = -1\r\n",
        "    depth = -1\r\n",
        "    def __init__(self, label, location, depth):\r\n",
        "       self.label = label\r\n",
        "       self.location = location\r\n",
        "       self.depth = depth\r\n",
        "\r\n",
        "def most_dangerous(arr):\r\n",
        "  if len(arr) > 0:\r\n",
        "    max_obj = arr[0]\r\n",
        "    for i in arr:\r\n",
        "      if danger_dict[labels_of_coco.index(i.label)] >= danger_dict[labels_of_coco.index(max_obj.label)]:\r\n",
        "        max_obj = i\r\n",
        "    return max_obj\r\n",
        "  return -1         \r\n",
        "#------------------------------------------------------       \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\"\"\"/////////////////////////////////////////////////\"\"\"\r\n",
        "def resize(path):\r\n",
        "  \"\"\"Takes a jpg, resize it to 480*640 png image\"\"\"\r\n",
        "  imageFile = path\r\n",
        "  img = cv2.imread(imageFile)\r\n",
        "  width = 480\r\n",
        "  height = 640\r\n",
        "  dim = (width, height)\r\n",
        "  img = cv2.resize(img, dim)   \r\n",
        "  cv2.imwrite(path, img)\r\n",
        "#------------------------------------------------------  \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\"\"\"/////////////////////////////////////////////////\"\"\"\r\n",
        "def depth_approx(path, centers):\r\n",
        "  img = cv2.imread(path)\r\n",
        "  depthes = [] \r\n",
        "  for axis in centers:\r\n",
        "    depth = img[axis[1], axis[0]][0]\r\n",
        "    depthes.append(depth)\r\n",
        "  return depthes \r\n",
        "#------------------------------------------------------\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\"\"\"/////////////////////////////////////////////////\"\"\"\r\n",
        "#Using the capture snippet and modify it to take a capture automatically\r\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\r\n",
        "  js = Javascript('''\r\n",
        "    async function takePhoto(quality) {\r\n",
        "      const div = document.createElement('div');\r\n",
        "\r\n",
        "      const video = document.createElement('video');\r\n",
        "      video.style.display = 'block';\r\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\r\n",
        "\r\n",
        "      document.body.appendChild(div);\r\n",
        "      div.appendChild(video);\r\n",
        "      video.srcObject = stream;\r\n",
        "      await video.play();\r\n",
        "\r\n",
        "      // Resize the output to fit the video element.\r\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\r\n",
        "      \r\n",
        "      const canvas = document.createElement('canvas');\r\n",
        "      canvas.width = video.videoWidth;\r\n",
        "      canvas.height = video.videoHeight;\r\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\r\n",
        "      stream.getVideoTracks()[0].stop();\r\n",
        "      div.remove();\r\n",
        "      return canvas.toDataURL('image/jpeg', quality);\r\n",
        "    }\r\n",
        "    ''')\r\n",
        "  display(js)\r\n",
        "  data = eval_js('takePhoto({})'.format(quality))\r\n",
        "  binary = b64decode(data.split(',')[1])\r\n",
        "  with open(filename, 'wb') as f:\r\n",
        "    f.write(binary)\r\n",
        "  return filename  \r\n",
        "#------------------------------------------------------\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\"\"\"/////////////////////////////////////////////////\"\"\"\r\n",
        "#Used to run the Colab-Audio service in a loop\r\n",
        "class _V6Server(socketserver.TCPServer):\r\n",
        "  address_family = socket.AF_INET6\r\n",
        "\r\n",
        "class _FileHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):\r\n",
        "  \"\"\"SimpleHTTPRequestHandler with a couple tweaks.\"\"\"\r\n",
        "\r\n",
        "  def translate_path(self, path):\r\n",
        "    # Client specifies absolute paths.\r\n",
        "    return path\r\n",
        "\r\n",
        "  def log_message(self, fmt, *args):\r\n",
        "    # Suppress logging since it's on the background. Any errors will be reported\r\n",
        "    # via the handler.\r\n",
        "    pass\r\n",
        "\r\n",
        "  def end_headers(self):\r\n",
        "    # Do not cache the response in the notebook, since it may be quite large.\r\n",
        "    self.send_header('x-colab-notebook-cache-control', 'no-cache')\r\n",
        "    SimpleHTTPServer.SimpleHTTPRequestHandler.end_headers(self)\r\n",
        "\r\n",
        "\r\n",
        "def play_audio(filename):\r\n",
        "  \"\"\"Downloads the file to the user's local disk via a browser download action.\r\n",
        "\r\n",
        "  Args:\r\n",
        "    filename: Name of the file on disk to be downloaded.\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  started = threading.Event()\r\n",
        "  port = portpicker.pick_unused_port()\r\n",
        "\r\n",
        "  def server_entry():\r\n",
        "    httpd = _V6Server(('::', port), _FileHandler)\r\n",
        "    started.set()\r\n",
        "    # Serve multiple requests, in case the audio is played more than once.\r\n",
        "    httpd.serve_forever()\r\n",
        "\r\n",
        "  thread = threading.Thread(target=server_entry)\r\n",
        "  thread.start()\r\n",
        "  started.wait()\r\n",
        "\r\n",
        "  output.eval_js(\"\"\"\r\n",
        "    (()=> {\r\n",
        "      const audio = document.createElement('audio');\r\n",
        "      audio.controls = true;\r\n",
        "      audio.autoplay = true;\r\n",
        "      audio.src = `https://localhost:%(port)d%(path)s`;\r\n",
        "      document.body.appendChild(audio);\r\n",
        "    })()\r\n",
        "  \"\"\"% {'port': port, 'path': os.path.abspath(filename)})\r\n",
        "#------------------------------------------------------\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\"\"\"Load the MDE model\"\"\"\r\n",
        "# Keras / TensorFlow\r\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '5'\r\n",
        "\r\n",
        "# Custom object needed for inference and training\r\n",
        "custom_objects = {'BilinearUpSampling2D': BilinearUpSampling2D, 'depth_loss_function': None}\r\n",
        "\r\n",
        "print('Loading model...')\r\n",
        "# Load model into GPU / CPU\r\n",
        "model = load_model(\"/content/DenseDepth/nyu.h5\", custom_objects=custom_objects, compile=False)\r\n",
        "print('\\nModel loaded ({0}).'.format(\"/content/DenseDepth/nyu.h5\"))\r\n",
        "#------------------------------------------------------\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\"\"\"Load YOLO model\"\"\"\r\n",
        "# load the COCO class labels our YOLO model was trained on\r\n",
        "LABELS = open(\"/content/darknet/data/coco.names\").read().strip().split(\"\\n\")\r\n",
        "\r\n",
        "# load our YOLO object detector trained on COCO dataset (80 classes)\r\n",
        "print(\"[INFO] loading YOLO from disk...\")\r\n",
        "net = cv2.dnn.readNetFromDarknet(\"/content/darknet/cfg/yolov3.cfg\", \"/content/darknet/yolov3.weights\")\r\n",
        "\r\n",
        "# determine only the *output* layer names that we need from YOLO\r\n",
        "ln = net.getLayerNames()\r\n",
        "ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\r\n",
        "#------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k41RBCfXPnpM"
      },
      "source": [
        "#utils.py\r\n",
        "import numpy as np\r\n",
        "from PIL import Image\r\n",
        "\r\n",
        "def DepthNorm(x, maxDepth):\r\n",
        "    return maxDepth / x\r\n",
        "\r\n",
        "def predict(model, images, minDepth=10, maxDepth=1000, batch_size=2):\r\n",
        "    # Support multiple RGBs, one RGB image, even grayscale \r\n",
        "    if len(images.shape) < 3: images = np.stack((images,images,images), axis=2)\r\n",
        "    if len(images.shape) < 4: images = images.reshape((1, images.shape[0], images.shape[1], images.shape[2]))\r\n",
        "    # Compute predictions\r\n",
        "    predictions = model.predict(images, batch_size=batch_size)\r\n",
        "    # Put in expected range\r\n",
        "    return np.clip(DepthNorm(predictions, maxDepth=1000), minDepth, maxDepth) / maxDepth\r\n",
        "\r\n",
        "def scale_up(scale, images):\r\n",
        "    from skimage.transform import resize\r\n",
        "    scaled = []\r\n",
        "    \r\n",
        "    for i in range(len(images)):\r\n",
        "        img = images[i]\r\n",
        "        output_shape = (scale * img.shape[0], scale * img.shape[1])\r\n",
        "        scaled.append( resize(img, output_shape, order=1, preserve_range=True, mode='reflect', anti_aliasing=True ) )\r\n",
        "\r\n",
        "    return np.stack(scaled)\r\n",
        "\r\n",
        "def load_images(image_files):\r\n",
        "    loaded_images = []\r\n",
        "    for file in image_files:\r\n",
        "        x = np.clip(np.asarray(Image.open( file ), dtype=float) / 255, 0, 1)\r\n",
        "        loaded_images.append(x)\r\n",
        "    return np.stack(loaded_images, axis=0)\r\n",
        "\r\n",
        "def to_multichannel(i):\r\n",
        "    if i.shape[2] == 3: return i\r\n",
        "    i = i[:,:,0]\r\n",
        "    return np.stack((i,i,i), axis=2)\r\n",
        "        \r\n",
        "def display_images(outputs, inputs=None, gt=None, is_colormap=True, is_rescale=True):\r\n",
        "    import matplotlib.pyplot as plt\r\n",
        "    import skimage\r\n",
        "    from skimage.transform import resize\r\n",
        "\r\n",
        "    plasma = plt.get_cmap('plasma')\r\n",
        "\r\n",
        "    shape = (outputs[0].shape[0], outputs[0].shape[1], 3)\r\n",
        "    \r\n",
        "    all_images = []\r\n",
        "\r\n",
        "    for i in range(outputs.shape[0]):\r\n",
        "        imgs = []\r\n",
        "        \r\n",
        "        if isinstance(inputs, (list, tuple, np.ndarray)):\r\n",
        "            x = to_multichannel(inputs[i])\r\n",
        "            x = resize(x, shape, preserve_range=True, mode='reflect', anti_aliasing=True )\r\n",
        "            imgs.append(x)\r\n",
        "\r\n",
        "        if isinstance(gt, (list, tuple, np.ndarray)):\r\n",
        "            x = to_multichannel(gt[i])\r\n",
        "            x = resize(x, shape, preserve_range=True, mode='reflect', anti_aliasing=True )\r\n",
        "            imgs.append(x)\r\n",
        "\r\n",
        "        if is_colormap:\r\n",
        "            rescaled = outputs[i][:,:,0]\r\n",
        "            if is_rescale:\r\n",
        "                rescaled = rescaled - np.min(rescaled)\r\n",
        "                rescaled = rescaled / np.max(rescaled)\r\n",
        "            imgs.append(plasma(rescaled)[:,:,:3])\r\n",
        "        else:\r\n",
        "            imgs.append(to_multichannel(outputs[i]))\r\n",
        "\r\n",
        "        img_set = np.hstack(imgs)\r\n",
        "        all_images.append(img_set)\r\n",
        "\r\n",
        "    all_images = np.stack(all_images)\r\n",
        "    \r\n",
        "    return skimage.util.montage(all_images, multichannel=True, fill=(0,0,0))\r\n",
        "\r\n",
        "def save_images(filename, outputs, inputs=None, gt=None, is_colormap=True, is_rescale=False):\r\n",
        "    montage =  display_images(outputs, inputs, is_colormap, is_rescale)\r\n",
        "    im = Image.fromarray(np.uint8(montage*255))\r\n",
        "    im.save(filename)\r\n",
        "\r\n",
        "def load_test_data(test_data_zip_file='nyu_test.zip'):\r\n",
        "    print('Loading test data...', end='')\r\n",
        "    import numpy as np\r\n",
        "    from data import extract_zip\r\n",
        "    data = extract_zip(test_data_zip_file)\r\n",
        "    from io import BytesIO\r\n",
        "    rgb = np.load(BytesIO(data['eigen_test_rgb.npy']))\r\n",
        "    depth = np.load(BytesIO(data['eigen_test_depth.npy']))\r\n",
        "    crop = np.load(BytesIO(data['eigen_test_crop.npy']))\r\n",
        "    print('Test data loaded.\\n')\r\n",
        "    return {'rgb':rgb, 'depth':depth, 'crop':crop}\r\n",
        "\r\n",
        "def compute_errors(gt, pred):\r\n",
        "    thresh = np.maximum((gt / pred), (pred / gt))\r\n",
        "    a1 = (thresh < 1.25   ).mean()\r\n",
        "    a2 = (thresh < 1.25 ** 2).mean()\r\n",
        "    a3 = (thresh < 1.25 ** 3).mean()\r\n",
        "    abs_rel = np.mean(np.abs(gt - pred) / gt)\r\n",
        "    rmse = (gt - pred) ** 2\r\n",
        "    rmse = np.sqrt(rmse.mean())\r\n",
        "    log_10 = (np.abs(np.log10(gt)-np.log10(pred))).mean()\r\n",
        "    return a1, a2, a3, abs_rel, rmse, log_10\r\n",
        "\r\n",
        "def evaluate(model, rgb, depth, crop, batch_size=6, verbose=False):\r\n",
        "    N = len(rgb)\r\n",
        "\r\n",
        "    bs = batch_size\r\n",
        "\r\n",
        "    predictions = []\r\n",
        "    testSetDepths = []\r\n",
        "    \r\n",
        "    for i in range(N//bs):    \r\n",
        "        x = rgb[(i)*bs:(i+1)*bs,:,:,:]\r\n",
        "        \r\n",
        "        # Compute results\r\n",
        "        true_y = depth[(i)*bs:(i+1)*bs,:,:]\r\n",
        "        pred_y = scale_up(2, predict(model, x/255, minDepth=10, maxDepth=1000, batch_size=bs)[:,:,:,0]) * 10.0\r\n",
        "        \r\n",
        "        # Test time augmentation: mirror image estimate\r\n",
        "        pred_y_flip = scale_up(2, predict(model, x[...,::-1,:]/255, minDepth=10, maxDepth=1000, batch_size=bs)[:,:,:,0]) * 10.0\r\n",
        "\r\n",
        "        # Crop based on Eigen et al. crop\r\n",
        "        true_y = true_y[:,crop[0]:crop[1]+1, crop[2]:crop[3]+1]\r\n",
        "        pred_y = pred_y[:,crop[0]:crop[1]+1, crop[2]:crop[3]+1]\r\n",
        "        pred_y_flip = pred_y_flip[:,crop[0]:crop[1]+1, crop[2]:crop[3]+1]\r\n",
        "        \r\n",
        "        # Compute errors per image in batch\r\n",
        "        for j in range(len(true_y)):\r\n",
        "            predictions.append(   (0.5 * pred_y[j]) + (0.5 * np.fliplr(pred_y_flip[j]))   )\r\n",
        "            testSetDepths.append(   true_y[j]   )\r\n",
        "\r\n",
        "    predictions = np.stack(predictions, axis=0)\r\n",
        "    testSetDepths = np.stack(testSetDepths, axis=0)\r\n",
        "\r\n",
        "    e = compute_errors(predictions, testSetDepths)\r\n",
        "\r\n",
        "    if verbose:\r\n",
        "        print(\"{:>10}, {:>10}, {:>10}, {:>10}, {:>10}, {:>10}\".format('a1', 'a2', 'a3', 'rel', 'rms', 'log_10'))\r\n",
        "        print(\"{:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}\".format(e[0],e[1],e[2],e[3],e[4],e[5]))\r\n",
        "\r\n",
        "    return e"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTYuUYJRPtlM"
      },
      "source": [
        "# ROOAA App"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnUyas84PvKP"
      },
      "source": [
        "while True: \r\n",
        "  \"\"\"-------------------------Take a picture-------------------------\"\"\"\r\n",
        "  try:\r\n",
        "    filename = take_photo()\r\n",
        "  except Exception as err:\r\n",
        "    # Errors will be thrown if the user does not have a webcam or if they do not\r\n",
        "    # grant the page permission to access it.\r\n",
        "    print(str(err))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  \"\"\"-------------------------YOLO obj detection and location-------------------------\"\"\"\r\n",
        "  resize(\"/content/photo.jpg\")\r\n",
        "  classes, locations, centers = detect_and_locate_obj('/content/photo.jpg', LABELS, net, ln)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  \"\"\"-------------------------Estimate the distance-------------------------\"\"\"\r\n",
        "  inputs = load_images( glob.glob(\"/content/photo.jpg\") )\r\n",
        "\r\n",
        "  outputs = predict(model, inputs)\r\n",
        "\r\n",
        "  save_images('/content/output.jpg', outputs)\r\n",
        "\r\n",
        "  resize('/content/output.jpg')\r\n",
        "\r\n",
        "  depthes = depth_approx(\"/content/output.jpg\", centers)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  \"\"\"-------------------------filtration-------------------------\"\"\"\r\n",
        "  filtration_arr = []\r\n",
        "  warning_area = []\r\n",
        "  informing_area = []\r\n",
        "  left_objs = []\r\n",
        "  right_objs = []\r\n",
        "  center_objs = []\r\n",
        "  texts = \"\"\r\n",
        "  #---------------------------------\r\n",
        "  for i in range(len(classes)):\r\n",
        "    obj = filtration(classes[i], locations[i], depthes[i])\r\n",
        "    filtration_arr.append(obj)\r\n",
        "  #---------------------------------\r\n",
        "  for i in filtration_arr:\r\n",
        "    if i.depth <= 35:\r\n",
        "      warning_area.append(i)   \r\n",
        "    else:\r\n",
        "      informing_area.append(i)   \r\n",
        "  #---------------------------------\r\n",
        "  if  len(warning_area) > 0:\r\n",
        "    for i in warning_area:\r\n",
        "      if i.location == 'center ':\r\n",
        "        center_objs.append(i)\r\n",
        "      elif i.location =='left ':  \r\n",
        "        left_objs.append(i)\r\n",
        "      elif i.location == 'right ':  \r\n",
        "        right_objs.append(i)\r\n",
        "\r\n",
        "    most_dangerous_center = most_dangerous(center_objs)\r\n",
        "    most_dangerous_left = most_dangerous(left_objs)\r\n",
        "    most_dangerous_right = most_dangerous(right_objs)\r\n",
        "\r\n",
        "    texts =  \"Watch out \"\r\n",
        "    if most_dangerous_center != -1  :\r\n",
        "      texts += most_dangerous_center.location + \" \" + most_dangerous_center.label + \" \"\r\n",
        "\r\n",
        "    if most_dangerous_left != -1:\r\n",
        "      texts += most_dangerous_left.location + \" \" + most_dangerous_left.label + \" \"\r\n",
        "   \r\n",
        "    if most_dangerous_right != -1:\r\n",
        "      texts += most_dangerous_right.location + \" \" + most_dangerous_right.label + \" \"\r\n",
        "  #---------------------------------\r\n",
        "  elif len(informing_area) > 0:   \r\n",
        "    for i in informing_area:\r\n",
        "      if i.location == 'center ':\r\n",
        "        center_objs.append(i)\r\n",
        "      elif i.location =='left ':  \r\n",
        "        left_objs.append(i)\r\n",
        "      elif i.location == 'right ':  \r\n",
        "        right_objs.append(i)\r\n",
        "\r\n",
        "    most_dangerous_center = most_dangerous(center_objs)\r\n",
        "    most_dangerous_left = most_dangerous(left_objs)\r\n",
        "    most_dangerous_right = most_dangerous(right_objs)\r\n",
        "\r\n",
        "    texts =  \" \"\r\n",
        "    if most_dangerous_center != -1  :\r\n",
        "      texts += most_dangerous_center.location + \" \" + most_dangerous_center.label + \" \"\r\n",
        "\r\n",
        "    if most_dangerous_left != -1:\r\n",
        "      texts += most_dangerous_left.location + \" \" + most_dangerous_left.label + \" \"\r\n",
        "   \r\n",
        "    if most_dangerous_right != -1:\r\n",
        "      texts += most_dangerous_right.location + \" \" + most_dangerous_right.label + \" \"\r\n",
        "  #---------------------------------\r\n",
        "\r\n",
        "  \r\n",
        "\r\n",
        "  \"\"\"-------------------------Convert text to speech-------------------------\"\"\"\r\n",
        "  if not texts:\r\n",
        "    texts = \"Nothing\"\r\n",
        "\r\n",
        "  tts = gTTS(text=texts, lang='en')\r\n",
        "\r\n",
        "  tts.save(\"ttsFile.mp3\")  \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  \"\"\"-------------------------Play Audio-------------------------\"\"\"\r\n",
        "  play_audio(\"/content/ttsFile.mp3\")\r\n",
        "  time.sleep(1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}